{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62dd9bd5-21ee-4846-a873-077e25d0e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943f2a9-1d6a-4360-bb21-4b2ec3ac7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13666766-6a87-47bf-a58c-c1fc6e62d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53587c0-b3f0-47d8-a2db-905691f759be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].select(range(1000))\n",
    "ds.to_csv(\"gretel_data_1k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4319747b-94ce-4a42-bcc2-4b430b35639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be69d632-5be7-42f9-a8c2-c0b2dee6859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import yaml\n",
    "from typing import Tuple\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from llmtune.pydantic_models.config_model import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f1e997-66bf-441d-920f-0af7f454b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_HASH = \"9pQFRZ\"\n",
    "config_path = f\"./experiment/{EXPERIMENT_HASH}/config/config.yml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config = Config(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817f3da6-3d15-4526-8657-71d7cd1767a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee3e980f3bd497f8515f192658ece6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "weights_path = f\"./experiment/{EXPERIMENT_HASH}/weights/\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    weights_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "tok = AutoTokenizer.from_pretrained(weights_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0797ee20-b120-472a-be93-3c4e75973afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt: str, model, tok: AutoTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Outputs predicted sequence and probability\n",
    "    \"\"\"\n",
    "    input_ids = tok.encode(prompt, return_tensors=\"pt\", truncation=True).cuda()\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        )\n",
    "\n",
    "    gen_results = model.generate(input_ids, gen_config)\n",
    "    gen_tok_ids = gen_results.sequences.squeeze(0)[input_ids.shape[1]:]\n",
    "\n",
    "    seq = tok.decode(gen_tok_ids, skip_special_tokens=True)\n",
    "\n",
    "    seq_score = torch.stack(gen_results.scores).squeeze(1) # get raw output scores \n",
    "    seq_score = torch.log(torch.softmax(seq_score, dim=1)) # softmax and log to get log_prob\n",
    "    seq_score = seq_score.gather(dim=1, index=gen_tok_ids.view(-1, 1)).squeeze()\n",
    "    seq_score = torch.exp(torch.sum(seq_score)).item() # sum and exp to get prob\n",
    "\n",
    "    return seq, seq_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f48d32a-2eb9-4d32-8b7f-6ccb04790473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1/500\n",
      "Done 2/500\n",
      "Done 3/500\n",
      "Done 4/500\n",
      "Done 5/500\n",
      "Done 6/500\n",
      "Done 7/500\n",
      "Done 8/500\n",
      "Done 9/500\n",
      "Done 10/500\n",
      "Done 11/500\n",
      "Done 12/500\n",
      "Done 13/500\n",
      "Done 14/500\n",
      "Done 15/500\n",
      "Done 16/500\n",
      "Done 17/500\n",
      "Done 18/500\n",
      "Done 19/500\n",
      "Done 20/500\n",
      "Done 21/500\n",
      "Done 22/500\n",
      "Done 23/500\n",
      "Done 24/500\n",
      "Done 25/500\n",
      "Done 26/500\n",
      "Done 27/500\n",
      "Done 28/500\n",
      "Done 29/500\n",
      "Done 30/500\n",
      "Done 31/500\n",
      "Done 32/500\n",
      "Done 33/500\n",
      "Done 34/500\n",
      "Done 35/500\n",
      "Done 36/500\n",
      "Done 37/500\n",
      "Done 38/500\n",
      "Done 39/500\n",
      "Done 40/500\n",
      "Done 41/500\n",
      "Done 42/500\n",
      "Done 43/500\n",
      "Done 44/500\n",
      "Done 45/500\n",
      "Done 46/500\n",
      "Done 47/500\n",
      "Done 48/500\n",
      "Done 49/500\n",
      "Done 50/500\n",
      "Done 51/500\n",
      "Done 52/500\n",
      "Done 53/500\n",
      "Done 54/500\n",
      "Done 55/500\n",
      "Done 56/500\n",
      "Done 57/500\n",
      "Done 58/500\n",
      "Done 59/500\n",
      "Done 60/500\n",
      "Done 61/500\n",
      "Done 62/500\n",
      "Done 63/500\n",
      "Done 64/500\n",
      "Done 65/500\n",
      "Done 66/500\n",
      "Done 67/500\n",
      "Done 68/500\n",
      "Done 69/500\n",
      "Done 70/500\n",
      "Done 71/500\n",
      "Done 72/500\n",
      "Done 73/500\n",
      "Done 74/500\n",
      "Done 75/500\n",
      "Done 76/500\n",
      "Done 77/500\n",
      "Done 78/500\n",
      "Done 79/500\n",
      "Done 80/500\n",
      "Done 81/500\n",
      "Done 82/500\n",
      "Done 83/500\n",
      "Done 84/500\n",
      "Done 85/500\n",
      "Done 86/500\n",
      "Done 87/500\n",
      "Done 88/500\n",
      "Done 89/500\n",
      "Done 90/500\n",
      "Done 91/500\n",
      "Done 92/500\n",
      "Done 93/500\n",
      "Done 94/500\n",
      "Done 95/500\n",
      "Done 96/500\n",
      "Done 97/500\n",
      "Done 98/500\n",
      "Done 99/500\n",
      "Done 100/500\n",
      "Done 101/500\n",
      "Done 102/500\n",
      "Done 103/500\n",
      "Done 104/500\n",
      "Done 105/500\n",
      "Done 106/500\n",
      "Done 107/500\n",
      "Done 108/500\n",
      "Done 109/500\n",
      "Done 110/500\n",
      "Done 111/500\n",
      "Done 112/500\n",
      "Done 113/500\n",
      "Done 114/500\n",
      "Done 115/500\n",
      "Done 116/500\n",
      "Done 117/500\n",
      "Done 118/500\n",
      "Done 119/500\n",
      "Done 120/500\n",
      "Done 121/500\n",
      "Done 122/500\n",
      "Done 123/500\n",
      "Done 124/500\n",
      "Done 125/500\n",
      "Done 126/500\n",
      "Done 127/500\n",
      "Done 128/500\n",
      "Done 129/500\n",
      "Done 130/500\n",
      "Done 131/500\n",
      "Done 132/500\n",
      "Done 133/500\n",
      "Done 134/500\n",
      "Done 135/500\n",
      "Done 136/500\n",
      "Done 137/500\n",
      "Done 138/500\n",
      "Done 139/500\n",
      "Done 140/500\n",
      "Done 141/500\n",
      "Done 142/500\n",
      "Done 143/500\n",
      "Done 144/500\n",
      "Done 145/500\n",
      "Done 146/500\n",
      "Done 147/500\n",
      "Done 148/500\n",
      "Done 149/500\n",
      "Done 150/500\n",
      "Done 151/500\n",
      "Done 152/500\n",
      "Done 153/500\n",
      "Done 154/500\n",
      "Done 155/500\n",
      "Done 156/500\n",
      "Done 157/500\n",
      "Done 158/500\n",
      "Done 159/500\n",
      "Done 160/500\n",
      "Done 161/500\n",
      "Done 162/500\n",
      "Done 163/500\n",
      "Done 164/500\n",
      "Done 165/500\n",
      "Done 166/500\n",
      "Done 167/500\n",
      "Done 168/500\n",
      "Done 169/500\n",
      "Done 170/500\n",
      "Done 171/500\n",
      "Done 172/500\n",
      "Done 173/500\n",
      "Done 174/500\n",
      "Done 175/500\n",
      "Done 176/500\n",
      "Done 177/500\n",
      "Done 178/500\n",
      "Done 179/500\n",
      "Done 180/500\n",
      "Done 181/500\n",
      "Done 182/500\n",
      "Done 183/500\n",
      "Done 184/500\n",
      "Done 185/500\n",
      "Done 186/500\n",
      "Done 187/500\n",
      "Done 188/500\n",
      "Done 189/500\n",
      "Done 190/500\n",
      "Done 191/500\n",
      "Done 192/500\n",
      "Done 193/500\n",
      "Done 194/500\n",
      "Done 195/500\n",
      "Done 196/500\n",
      "Done 197/500\n",
      "Done 198/500\n",
      "Done 199/500\n",
      "Done 200/500\n",
      "Done 201/500\n",
      "Done 202/500\n",
      "Done 203/500\n",
      "Done 204/500\n",
      "Done 205/500\n",
      "Done 206/500\n",
      "Done 207/500\n",
      "Done 208/500\n",
      "Done 209/500\n",
      "Done 210/500\n",
      "Done 211/500\n",
      "Done 212/500\n",
      "Done 213/500\n",
      "Done 214/500\n",
      "Done 215/500\n",
      "Done 216/500\n",
      "Done 217/500\n",
      "Done 218/500\n",
      "Done 219/500\n",
      "Done 220/500\n",
      "Done 221/500\n",
      "Done 222/500\n",
      "Done 223/500\n",
      "Done 224/500\n",
      "Done 225/500\n",
      "Done 226/500\n",
      "Done 227/500\n",
      "Done 228/500\n",
      "Done 229/500\n",
      "Done 230/500\n",
      "Done 231/500\n",
      "Done 232/500\n",
      "Done 233/500\n",
      "Done 234/500\n",
      "Done 235/500\n",
      "Done 236/500\n",
      "Done 237/500\n",
      "Done 238/500\n",
      "Done 239/500\n",
      "Done 240/500\n",
      "Done 241/500\n",
      "Done 242/500\n",
      "Done 243/500\n",
      "Done 244/500\n",
      "Done 245/500\n",
      "Done 246/500\n",
      "Done 247/500\n",
      "Done 248/500\n",
      "Done 249/500\n",
      "Done 250/500\n",
      "Done 251/500\n",
      "Done 252/500\n",
      "Done 253/500\n",
      "Done 254/500\n",
      "Done 255/500\n",
      "Done 256/500\n",
      "Done 257/500\n",
      "Done 258/500\n",
      "Done 259/500\n",
      "Done 260/500\n",
      "Done 261/500\n",
      "Done 262/500\n",
      "Done 263/500\n",
      "Done 264/500\n",
      "Done 265/500\n",
      "Done 266/500\n",
      "Done 267/500\n",
      "Done 268/500\n",
      "Done 269/500\n",
      "Done 270/500\n",
      "Done 271/500\n",
      "Done 272/500\n",
      "Done 273/500\n",
      "Done 274/500\n",
      "Done 275/500\n",
      "Done 276/500\n",
      "Done 277/500\n",
      "Done 278/500\n",
      "Done 279/500\n",
      "Done 280/500\n",
      "Done 281/500\n",
      "Done 282/500\n",
      "Done 283/500\n",
      "Done 284/500\n",
      "Done 285/500\n",
      "Done 286/500\n",
      "Done 287/500\n",
      "Done 288/500\n",
      "Done 289/500\n",
      "Done 290/500\n",
      "Done 291/500\n",
      "Done 292/500\n",
      "Done 293/500\n",
      "Done 294/500\n",
      "Done 295/500\n",
      "Done 296/500\n",
      "Done 297/500\n",
      "Done 298/500\n",
      "Done 299/500\n",
      "Done 300/500\n",
      "Done 301/500\n",
      "Done 302/500\n",
      "Done 303/500\n",
      "Done 304/500\n",
      "Done 305/500\n",
      "Done 306/500\n",
      "Done 307/500\n",
      "Done 308/500\n",
      "Done 309/500\n",
      "Done 310/500\n",
      "Done 311/500\n",
      "Done 312/500\n",
      "Done 313/500\n",
      "Done 314/500\n",
      "Done 315/500\n",
      "Done 316/500\n",
      "Done 317/500\n",
      "Done 318/500\n",
      "Done 319/500\n",
      "Done 320/500\n",
      "Done 321/500\n",
      "Done 322/500\n",
      "Done 323/500\n",
      "Done 324/500\n",
      "Done 325/500\n",
      "Done 326/500\n",
      "Done 327/500\n",
      "Done 328/500\n",
      "Done 329/500\n",
      "Done 330/500\n",
      "Done 331/500\n",
      "Done 332/500\n",
      "Done 333/500\n",
      "Done 334/500\n",
      "Done 335/500\n",
      "Done 336/500\n",
      "Done 337/500\n",
      "Done 338/500\n",
      "Done 339/500\n",
      "Done 340/500\n",
      "Done 341/500\n",
      "Done 342/500\n",
      "Done 343/500\n",
      "Done 344/500\n",
      "Done 345/500\n",
      "Done 346/500\n",
      "Done 347/500\n",
      "Done 348/500\n",
      "Done 349/500\n",
      "Done 350/500\n",
      "Done 351/500\n",
      "Done 352/500\n",
      "Done 353/500\n",
      "Done 354/500\n",
      "Done 355/500\n",
      "Done 356/500\n",
      "Done 357/500\n",
      "Done 358/500\n",
      "Done 359/500\n",
      "Done 360/500\n",
      "Done 361/500\n",
      "Done 362/500\n",
      "Done 363/500\n",
      "Done 364/500\n",
      "Done 365/500\n",
      "Done 366/500\n",
      "Done 367/500\n",
      "Done 368/500\n",
      "Done 369/500\n",
      "Done 370/500\n",
      "Done 371/500\n",
      "Done 372/500\n",
      "Done 373/500\n",
      "Done 374/500\n",
      "Done 375/500\n",
      "Done 376/500\n",
      "Done 377/500\n",
      "Done 378/500\n",
      "Done 379/500\n",
      "Done 380/500\n",
      "Done 381/500\n",
      "Done 382/500\n",
      "Done 383/500\n",
      "Done 384/500\n",
      "Done 385/500\n",
      "Done 386/500\n",
      "Done 387/500\n",
      "Done 388/500\n",
      "Done 389/500\n",
      "Done 390/500\n",
      "Done 391/500\n",
      "Done 392/500\n",
      "Done 393/500\n",
      "Done 394/500\n",
      "Done 395/500\n",
      "Done 396/500\n",
      "Done 397/500\n",
      "Done 398/500\n",
      "Done 399/500\n",
      "Done 400/500\n",
      "Done 401/500\n",
      "Done 402/500\n",
      "Done 403/500\n",
      "Done 404/500\n",
      "Done 405/500\n",
      "Done 406/500\n",
      "Done 407/500\n",
      "Done 408/500\n",
      "Done 409/500\n",
      "Done 410/500\n",
      "Done 411/500\n",
      "Done 412/500\n",
      "Done 413/500\n",
      "Done 414/500\n",
      "Done 415/500\n",
      "Done 416/500\n",
      "Done 417/500\n",
      "Done 418/500\n",
      "Done 419/500\n",
      "Done 420/500\n",
      "Done 421/500\n",
      "Done 422/500\n",
      "Done 423/500\n",
      "Done 424/500\n",
      "Done 425/500\n",
      "Done 426/500\n",
      "Done 427/500\n",
      "Done 428/500\n",
      "Done 429/500\n",
      "Done 430/500\n",
      "Done 431/500\n",
      "Done 432/500\n",
      "Done 433/500\n",
      "Done 434/500\n",
      "Done 435/500\n",
      "Done 436/500\n",
      "Done 437/500\n",
      "Done 438/500\n",
      "Done 439/500\n",
      "Done 440/500\n",
      "Done 441/500\n",
      "Done 442/500\n",
      "Done 443/500\n",
      "Done 444/500\n",
      "Done 445/500\n",
      "Done 446/500\n",
      "Done 447/500\n",
      "Done 448/500\n",
      "Done 449/500\n",
      "Done 450/500\n",
      "Done 451/500\n",
      "Done 452/500\n",
      "Done 453/500\n",
      "Done 454/500\n",
      "Done 455/500\n",
      "Done 456/500\n",
      "Done 457/500\n",
      "Done 458/500\n",
      "Done 459/500\n",
      "Done 460/500\n",
      "Done 461/500\n",
      "Done 462/500\n",
      "Done 463/500\n",
      "Done 464/500\n",
      "Done 465/500\n",
      "Done 466/500\n",
      "Done 467/500\n",
      "Done 468/500\n",
      "Done 469/500\n",
      "Done 470/500\n",
      "Done 471/500\n",
      "Done 472/500\n",
      "Done 473/500\n",
      "Done 474/500\n",
      "Done 475/500\n",
      "Done 476/500\n",
      "Done 477/500\n",
      "Done 478/500\n",
      "Done 479/500\n",
      "Done 480/500\n",
      "Done 481/500\n",
      "Done 482/500\n",
      "Done 483/500\n",
      "Done 484/500\n",
      "Done 485/500\n",
      "Done 486/500\n",
      "Done 487/500\n",
      "Done 488/500\n",
      "Done 489/500\n",
      "Done 490/500\n",
      "Done 491/500\n",
      "Done 492/500\n",
      "Done 493/500\n",
      "Done 494/500\n",
      "Done 495/500\n",
      "Done 496/500\n",
      "Done 497/500\n",
      "Done 498/500\n",
      "Done 499/500\n",
      "Done 500/500\n"
     ]
    }
   ],
   "source": [
    "with open(\"prompts.csv\", mode='r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader)  # Read the header\n",
    "    data = [tuple(row) for row in reader]  # Read the data and convert each row to a tuple\n",
    "\n",
    "processed_preds = {}\n",
    "for idx, row in enumerate(data):\n",
    "    out, _ = infer(row[1], model, tok)\n",
    "    out = out.strip()\n",
    "    out += f\"\\t----- bird -----\\t{data[idx][0]}\"\n",
    "    processed_preds[str(idx)] = out\n",
    "    print(f\"Done {idx+1}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ca7077-44b3-4647-8aeb-6aaadf4c7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"predict_mini_dev_mistral-gretel-2_SQLite.json\", 'w') as json_file:\n",
    "    json.dump(processed_preds, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc290a-8077-4ff1-938a-4686b4849a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
